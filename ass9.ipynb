{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/78kKhrjeTLf1isL8wtsP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruhaan0001/CognitiveComputing/blob/main/ass9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports,\n",
        "technology, food, books, etc.).\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords).\n"
      ],
      "metadata": {
        "id": "vBntrpOMTyzy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arUAatPxTuIj",
        "outputId": "5a54bd8c-2b14-4291-e995-a9a8563f7ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus   import stopwords\n",
        "from nltk.stem     import PorterStemmer, LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Define the paragraph\n",
        "mytxt = \"\"\"From DBMS and DSA grind to debugging marathons, this semester tests your limits. Assignments pile up, coding contests become routine, and group projects feel like startup sprints. Amid the chaos, that one friend saves you—with Maggi, memes, and moral support. Then comes the unexpected twist: a crush—distracting yet thrilling. But in the end, chai breaks and hostel chaos make it all worthwhile.\"\"\"\n",
        "\n",
        "\n",
        "# Split into lines\n",
        "lines = mytxt.split(\"\\n\")\n",
        "\n",
        "# Display formatted text\n",
        "display(Markdown(mytxt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "-ZJD29DQULjy",
        "outputId": "78bbde02-ed54-4985-bf6c-8258aef41510"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "From DBMS and DSA grind to debugging marathons, this semester tests your limits. Assignments pile up, coding contests become routine, and group projects feel like startup sprints. Amid the chaos, that one friend saves you—with Maggi, memes, and moral support. Then comes the unexpected twist: a crush—distracting yet thrilling. But in the end, chai breaks and hostel chaos make it all worthwhile."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1:Convert to lowercase and remove punctuation\n",
        "text = mytxt.lower()\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Step 2: Tokenize\n",
        "word_tokens = word_tokenize(text)\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "\n",
        "# Step 3: Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
        "\n",
        "# Step 4: Frequency Distribution\n",
        "fd = FreqDist(filtered_words)\n",
        "print(\"Filtered Words:\", filtered_words)\n",
        "print(\"Word Frequencies:\")\n",
        "fd.pprint()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQKkat-NUr-p",
        "outputId": "368c7391-707d-4a5b-c487-4131dd37fe9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['dbms', 'dsa', 'grind', 'debugging', 'marathons', 'semester', 'tests', 'limits', 'assignments', 'pile', 'coding', 'contests', 'become', 'routine', 'group', 'projects', 'feel', 'like', 'startup', 'sprints', 'amid', 'chaos', 'one', 'friend', 'saves', 'you—with', 'maggi', 'memes', 'moral', 'support', 'comes', 'unexpected', 'twist', 'crush—distracting', 'yet', 'thrilling', 'end', 'chai', 'breaks', 'hostel', 'chaos', 'make', 'worthwhile']\n",
            "Word Frequencies:\n",
            "FreqDist({'chaos': 2, 'dbms': 1, 'dsa': 1, 'grind': 1, 'debugging': 1, 'marathons': 1, 'semester': 1, 'tests': 1, 'limits': 1, 'assignments': 1, ...})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Use filtered_words from Q1\n",
        "porter_result = [porter.stem(word) for word in filtered_words]\n",
        "lancaster_result = [lancaster.stem(word) for word in filtered_words]\n",
        "lemma_result = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "\n",
        "print(\"Porter Stemmer:\", porter_result)\n",
        "print(\"\\nLancaster Stemmer:\", lancaster_result)\n",
        "print(\"\\nLemmatized:\", lemma_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLfHQzWmUvzi",
        "outputId": "bed210a5-4c34-49e5-b881-1e4c743c517f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer: ['dbm', 'dsa', 'grind', 'debug', 'marathon', 'semest', 'test', 'limit', 'assign', 'pile', 'code', 'contest', 'becom', 'routin', 'group', 'project', 'feel', 'like', 'startup', 'sprint', 'amid', 'chao', 'one', 'friend', 'save', 'you—with', 'maggi', 'meme', 'moral', 'support', 'come', 'unexpect', 'twist', 'crush—distract', 'yet', 'thrill', 'end', 'chai', 'break', 'hostel', 'chao', 'make', 'worthwhil']\n",
            "\n",
            "Lancaster Stemmer: ['dbms', 'dsa', 'grind', 'debug', 'marathon', 'semest', 'test', 'limit', 'assign', 'pil', 'cod', 'contest', 'becom', 'routin', 'group', 'project', 'feel', 'lik', 'startup', 'sprints', 'amid', 'chao', 'on', 'friend', 'sav', 'you—with', 'magg', 'mem', 'mor', 'support', 'com', 'unexpect', 'twist', 'crush—distracting', 'yet', 'thrilling', 'end', 'cha', 'break', 'hostel', 'chao', 'mak', 'worthwhil']\n",
            "\n",
            "Lemmatized: ['dbms', 'dsa', 'grind', 'debugging', 'marathon', 'semester', 'test', 'limit', 'assignment', 'pile', 'coding', 'contest', 'become', 'routine', 'group', 'project', 'feel', 'like', 'startup', 'sprint', 'amid', 'chaos', 'one', 'friend', 'save', 'you—with', 'maggi', 'meme', 'moral', 'support', 'come', 'unexpected', 'twist', 'crush—distracting', 'yet', 'thrilling', 'end', 'chai', 'break', 'hostel', 'chaos', 'make', 'worthwhile']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Original text before preprocessing\n",
        "original_text = mytxt\n",
        "\n",
        "# a. Words with more than 5 letters\n",
        "long_words = re.findall(r'\\b\\w{6,}\\b', original_text)\n",
        "\n",
        "# b. Numbers (if any)\n",
        "numbers = re.findall(r'\\d+', original_text)\n",
        "\n",
        "# c. Capitalized words\n",
        "capitalized = re.findall(r'\\b[A-Z][a-z]*\\b', original_text)\n",
        "\n",
        "# d. Words with only alphabets\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', original_text)\n",
        "\n",
        "# e. Words starting with a vowel\n",
        "vowel_words = [word for word in alpha_words if word[0].lower() in 'aeiou']\n",
        "\n",
        "print(\"Words > 5 letters:\", long_words)\n",
        "print(\"\\nNumbers:\", numbers)\n",
        "print(\"\\nCapitalized words:\", capitalized)\n",
        "print(\"\\nAlphabet-only words:\", alpha_words)\n",
        "print(\"\\nWords starting with vowels:\", vowel_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KMLGhknU2xR",
        "outputId": "336f6965-8806-4893-c707-14542b61478c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words > 5 letters: ['debugging', 'marathons', 'semester', 'limits', 'Assignments', 'coding', 'contests', 'become', 'routine', 'projects', 'startup', 'sprints', 'friend', 'support', 'unexpected', 'distracting', 'thrilling', 'breaks', 'hostel', 'worthwhile']\n",
            "\n",
            "Numbers: []\n",
            "\n",
            "Capitalized words: ['From', 'Assignments', 'Amid', 'Maggi', 'Then', 'But']\n",
            "\n",
            "Alphabet-only words: ['From', 'DBMS', 'and', 'DSA', 'grind', 'to', 'debugging', 'marathons', 'this', 'semester', 'tests', 'your', 'limits', 'Assignments', 'pile', 'up', 'coding', 'contests', 'become', 'routine', 'and', 'group', 'projects', 'feel', 'like', 'startup', 'sprints', 'Amid', 'the', 'chaos', 'that', 'one', 'friend', 'saves', 'you', 'with', 'Maggi', 'memes', 'and', 'moral', 'support', 'Then', 'comes', 'the', 'unexpected', 'twist', 'a', 'crush', 'distracting', 'yet', 'thrilling', 'But', 'in', 'the', 'end', 'chai', 'breaks', 'and', 'hostel', 'chaos', 'make', 'it', 'all', 'worthwhile']\n",
            "\n",
            "Words starting with vowels: ['and', 'Assignments', 'up', 'and', 'Amid', 'one', 'and', 'unexpected', 'a', 'in', 'end', 'and', 'it', 'all']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text with emails, URLs, and phone numbers\n",
        "text_sample = \"Email me at test@example.com. Visit https://example.com or call +91 9876543210.\"\n",
        "\n",
        "# Custom tokenizer function\n",
        "def custom_tokenizer(text):\n",
        "    # Keep contractions, hyphens, decimal numbers\n",
        "    return re.findall(r\"\\b\\w+(?:[-']\\w+)*\\b|\\d+\\.\\d+|\\d+\", text)\n",
        "\n",
        "# Apply tokenizer\n",
        "tokens = custom_tokenizer(text_sample)\n",
        "\n",
        "# Replace email, URL, phone number with placeholders\n",
        "text_cleaned = re.sub(r'\\S+@\\S+', '<EMAIL>', text_sample)\n",
        "text_cleaned = re.sub(r'https?://\\S+', '<URL>', text_cleaned)\n",
        "text_cleaned = re.sub(r'\\+91\\s?\\d{10}|\\d{3}-\\d{3}-\\d{4}', '<PHONE>', text_cleaned)\n",
        "\n",
        "print(\"Custom Tokens:\", tokens)\n",
        "print(\"Cleaned Text:\", text_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX3Dlq-qU4LB",
        "outputId": "7195ce17-ea59-49c1-dbb6-d2c0fc607e75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokens: ['Email', 'me', 'at', 'test', 'example', 'com', 'Visit', 'https', 'example', 'com', 'or', 'call', '91', '9876543210']\n",
            "Cleaned Text: Email me at <EMAIL> Visit <URL> or call <PHONE>.\n"
          ]
        }
      ]
    }
  ]
}